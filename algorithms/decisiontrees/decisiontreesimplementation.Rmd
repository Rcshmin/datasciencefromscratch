---
title: "Decision Trees"
author: "Rcshmin"
date: '2022-07-05'
output:
  rmdformats::downcute:
    self_contained: yes
    thumbnails: no
    lightbox: yes
    gallery: no
    highlight: tango
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(readr)
library(tidyverse)
library(pilot)
library(showtext)
library(kableExtra)
library(gridExtra)
library(rpart)
library(rpart.plot)

#Global chunk datasets

salmon_fish <- read.csv(url("https://raw.githubusercontent.com/ritvikmath/YouTubeVideoCode/main/fish.csv"))


```

## What are decision trees?

**Decision trees are a non-parametric supervised learning method**. **<span style="color: red;">Supervised</span>** means that the input and output data is labelled. **<span style="color: blue;">Non-parametric</span>** means that no assumptions are made regarding the assumptions of the population. This definition is obviously not that useful, but with some further consideration we can make some sense of it. **<span style="color: green;">Analogous to a tree in real life</span>**, a decision tree is a **tree-like model of decisions**. In essence, we pass this model data on several input variables, and ask it to create a tree that **predicts** the **value** of the **target variable**. This is all probably best understood through an example, so let's take a look at one...

### Example

In this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some **patterns**, in how each type of fish is grouped based off its two **characteristics**. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point...

```{r, echo = F, out.width="100%", fig.align='center'}

#Prepare fonts

font_add_google("Montserrat", "montsr")
showtext_auto()

#Create scatter plot

plot1 <- ggplot(data = salmon_fish, aes(x = length, y = weight, color = type)) +
  geom_point() +
  labs(title = "Classifying fish by weight and length", 
       subtitle = "How can we partition this data into its classes?") +
  theme_classic() +
  theme(text=element_text(family="montsr"), 
        plot.title = element_text(size=32),
        plot.subtitle = element_text(size=22),
        axis.title = element_text(size=22),
        axis.text = element_text(size=16),
        legend.text = element_text(size=16), 
        legend.title = element_text(size=22)) +
  scale_color_pilot()

#Create gridarrange table

salmon_fish1 <- salmon_fish %>% 
  slice_head(n = 10)

salmon_fish2 <- tableGrob(salmon_fish1, rows=NULL)

grid.arrange(plot1, salmon_fish2, nrow=1, widths = c(2.75,1))

```

If we take all of these splits, or otherwise **decisions** and **organised them**, it would look something like this

```{r echo = F, fig.align='center'}

#Create decision tree on example data

salmon_tree <- rpart(type ~ length + weight, data = salmon_fish)

#Create decision tree visualization on example data

prp(salmon_tree, fallen.leaves = FALSE, type=1, extra=1, varlen=0, faclen=0, yesno.yshift=-1, cex = 1.5)

```

And voila, we have our tree! So from referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what  


```{r eval = F, include = F, fig.cap = "Decision tree on kyphosis following spinal surgery" }

include_graphics(c("https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png"))

```

```{r include = F, eval = F}
**<span style="color: green;">whether this patient has kyphosis?</span>**
```





---
title: "Decision Trees"
author: "Rcshmin"
date: '2022-07-05'
output:
  rmdformats::downcute:
    self_contained: yes
    thumbnails: no
    lightbox: yes
    gallery: no
    highlight: tango
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(readr)
library(tidyverse)
library(pilot)
library(showtext)
library(kableExtra)
library(gridExtra)
library(rpart)
library(rpart.plot)

#Global chunk datasets

salmon_fish <- read.csv(url("https://raw.githubusercontent.com/ritvikmath/YouTubeVideoCode/main/fish.csv"))


```

## What are decision trees?

Decision trees are a non-parametric supervised learning method. Supervised means that the input and output data is labelled. Non-parametric means that no assumptions are made regarding the assumptions of the population. This definition is obviously not that useful, but with some further consideration we can make some sense of it. Analogous to a tree in real life, a decision tree is a tree-like model of decisions. In essence, we pass this model data on several input variables, and ask it to create a tree that predicts the value of the target variable. This is all probably best understood through an example, so let's take a look at one...

### Example

In this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point...

```{r, echo = F, out.width="100%", fig.align='center'}

#Prepare fonts

font_add_google("Montserrat", "montsr")
showtext_auto()

#Create scatter plot

plot1 <- ggplot(data = salmon_fish, aes(x = length, y = weight, color = type)) +
  geom_point() +
  labs(title = "Classifying fish by weight and length", 
       subtitle = "How can we partition this data into its classes?") +
  theme_classic() +
  theme(text=element_text(family="montsr"), 
        plot.title = element_text(size=32),
        plot.subtitle = element_text(size=22),
        axis.title = element_text(size=22),
        axis.text = element_text(size=16),
        legend.text = element_text(size=16), 
        legend.title = element_text(size=22)) +
  scale_color_pilot()

#Create gridarrange table

salmon_fish1 <- salmon_fish %>% 
  slice_head(n = 10)

salmon_fish2 <- tableGrob(salmon_fish1, rows=NULL)

grid.arrange(plot1, salmon_fish2, nrow=1, widths = c(2.75,1))

```

If we take all of these splits, or otherwise decisions and organised them, it would look something like this

```{r echo = F, fig.align='center'}

#Create decision tree on example data

salmon_tree <- rpart(type ~ length + weight, data = salmon_fish)

#Create decision tree visualization on example data

prp(salmon_tree, fallen.leaves = FALSE, type=1, extra=1, varlen=0, faclen=0, yesno.yshift=-1, cex = 1.5)

```

And voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree's.

## Key Terminologies

It would good if we could have some terminology to describe the different parts of a tree...

* **Root Node**: The input data set or population used to create the decision tree is the root node. It is always the top most node in a decision tree from which the data set it split into different subsets
* **Decision node**: Sub-nodes that are split into further sub-nodes. The decision nodes, as the name suggests, are the nodes which split our data set into subsets. From our example before, the $\mbox{length} \geq 3$  or $\mbox{length }<7$ are two of the many decision nodes 
* **Parent and child node**: A node which is divided sub-nodes is the parent node of the sub-nodes, whereas the sub-nodes are the children of the given parent node. The $\mbox{weight} < 6.8$ is the parent node of the terminal nodes below it. 
* **Leaf/terminal node**: 





```{r eval = F, include = F, fig.cap = "Decision tree on kyphosis following spinal surgery" }

include_graphics(c("https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png"))

```

```{r include = F, eval = F}
**<span style="color: green;">whether this patient has kyphosis?</span>**
```





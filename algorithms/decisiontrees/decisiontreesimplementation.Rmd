---
title: "Decision Trees"
author: "Rcshmin"
date: '2022-07-05'
output:
  rmdformats::downcute:
    self_contained: yes
    thumbnails: no
    lightbox: yes
    gallery: no
    highlight: tango
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(readr)
library(tidyverse)
library(pilot)
library(showtext)
library(kableExtra)
library(gridExtra)
library(rpart)
library(rpart.plot)

#Global chunk datasets

salmon_fish <- read.csv(url("https://raw.githubusercontent.com/ritvikmath/YouTubeVideoCode/main/fish.csv"))

```

# What are decision trees?

Decision trees are a non-parametric supervised learning method. Supervised means that the input and output data is labelled. Non-parametric means that no assumptions are made regarding the assumptions of the population. This definition is obviously not that useful, but with some further consideration we can make some sense of it. Analogous to a tree in real life, a decision tree is a tree-like model of decisions. In essence, we pass this model data on several input variables, and ask it to create a tree that predicts the value of the target variable. This is all probably best understood through an example, so let's take a look at one...

## Example

In this example we consider how we can classify a fish as a salmon or tuna based off its length and weight. Looking at the below data set, it is easy to see some patterns, in how each type of fish is grouped based off its two characteristics. For instance, any fish with a length less than around 2.5 is a tuna. Continuing on, a fish with length greater than 2.5, and weight less than around 4 is a salmon. We could also say that a fish with length greater than 7.2 and weight greater than 4.1 is a tuna. You are probably getting the gist of it at this point...

```{r, echo = F, out.width="100%", fig.align='center'}

#Prepare fonts

font_add_google("Montserrat", "montsr")
showtext_auto()

#Create scatter plot

plot1 <- ggplot(data = salmon_fish, aes(x = length, y = weight, color = type)) +
  geom_point() +
  labs(title = "Classifying fish by weight and length", 
       subtitle = "How can we partition this data into its classes?") +
  theme_classic() +
  theme(text=element_text(family="montsr"), 
        plot.title = element_text(size=32),
        plot.subtitle = element_text(size=22),
        axis.title = element_text(size=22),
        axis.text = element_text(size=16),
        legend.text = element_text(size=16), 
        legend.title = element_text(size=22)) +
  scale_color_pilot()

#Create gridarrange table

salmon_fish1 <- salmon_fish %>% 
  slice_head(n = 10)

salmon_fish2 <- tableGrob(salmon_fish1, rows=NULL)

grid.arrange(plot1, salmon_fish2, nrow=1, widths = c(2.75,1))

```

If we take all of these splits, or otherwise decisions and organised them, it would look something like this

```{r echo = F, fig.align='center'}

#Create decision tree on example data

salmon_tree <- rpart(type ~ length + weight, data = salmon_fish)

#Create decision tree visualization on example data

prp(salmon_tree, fallen.leaves = FALSE, type=1, extra=1, varlen=0, faclen=0, yesno.yshift=-1, cex = 1.5)

```

And voila, we have our tree! From referring to the diagram above, we can see that the decision tree has created splits across our input variables (weight and length) such that our scatterplot becomes partitioned into rectangular regions containing each type of the fish (the target variable). This is exactly what the decision tree classifier does. Some of you may be asking why this is useful? Well suppose we have a noob fisherman in the Atlantic ocean (which only contains salmon and tuna for our purposes) who has just caught his first fish, but is unsure of what fish it is. Taking the decision tree just produced, this noob fisherman could then identify what fish he has caught. Well this is not exactly the most realistic scenario, but you get the idea hopefully. So now that we know what the algorithm does, lets take a look at some terms that are often thrown around when talking about decisions tree's.

# Describing a tree

It would good if we could have some terminology to describe the different parts of a tree...

* **Root Node**: The input data set or population used to create the decision tree is the root node. It is always the top most node in a decision tree from which the data set it split into different subsets
* **Decision node**: Sub-nodes that are split into further sub-nodes. The decision nodes, as the name suggests, are the nodes which split our data set into subsets. From our example before, the $\mbox{length} \geq 3$  or $\mbox{length }<7$ are two of the many decision nodes 
* **Parent and child node**: A node which is divided sub-nodes is the parent node of the sub-nodes, whereas the sub-nodes are the children of the given parent node. The $\mbox{weight} < 6.8$ is the parent node of the terminal nodes below it. 
* **Leaf/terminal node**: Nodes that do not get split any further
* **Pure node**: A node at which points classified all belong to a single class. For our tree, all the terminal nodes are also pure nodes
* **Branch/sub-tree**: A subsection of the entire tree is called branch or sub-tree


# Measures of uncertainty

We saw earlier that the purpose of a decision tree is to partition the data into regions of a certain class (or try its best to do so). There were many choices for the split among our two input variables, but one may naturally ask which of these splits are the best, and in which order should we carry them out, such that the resultant data is partitioned as good as possible. Now as it turns out, this question relates the notion of uncertainty. The best split will be the one that minimizes the uncertainty of the child nodes. More specifically, we seek to achieve the maximum level of homogeneity (sameness) in the target variable, such that the total uncertainty of the child nodes are less than the parent node. 

## Entropy

Entropy quantifies the amount of uncertainty involved in the outcome of a process. It has formula 
\begin{align*}
\mbox{Entropy} &= \sum_{c}{f_{c} \cdot I(c)}
\end{align*}

where $f_{c}$ is the fraction of a class in data set and $I(c)$ is the information content in the class. Also $c$ is the total number of classes. In the context of decision tree classifiers, $I(c) = -\log_{2}{(f_{c})}$ which gives

\begin{align*}
\mbox{Entropy} &= -\sum_{c}{f_{c} \cdot \log_{2}{(f_{c})}} & \\
\end{align*}

The choice of the $\mbox{log}$ function is beyond the scope of this article, but those interested may wish to take a look at [this article](https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c). Implementing an entropy function can be done as shown below

```{r include = T}
get_entropy <- function(x){
  #Assume x is factor of labels
  if(length(x) == 0) return(0)
  weights = table(x)/length(x)
  info_content = -weights*log2(weights)
  entropy = sum(info_content)
  return(entropy)
}
```

We can perform a few checks using our function to check that it performs as expected

```{r echo = T, collapse = T}
#Entropy is zero?
get_entropy(c(0,0,0,0,0))
#Entropy is one?
get_entropy(c(0,0,0,1,1,1))
#Entropy is non-zero?
get_entropy(salmon_fish$type)

```

If we only have one class then our data is homogeneous, which means there is no uncertainty regarding the data. If we have an equal number of observations across two classes, then uncertainty is at its maximum. Note that a lower value of entropy always means less uncertainty. A simple situation which may help one understand how entropy works is the flipping of a coin

```{r echo = F, warnings = F, message = F}
#Get entropy for probabilities

vals <- double(length = 10)
entrop <- double(length = 11)

for(i in 0:10){
  counts <- i
  while(counts != 0){
    vals[counts] <- 1
    counts <- counts - 1
  }
  entrop[i+1] <- get_entropy(vals)
}

entrop2 <- data.frame(seq(0,1,0.1), entrop)
names(entrop2) <- c("Probability", "Entropy")

#Create graph

ggplot(data = entrop2, aes(x = Probability, y = Entropy)) +
  geom_point(size = 2) + 
  stat_smooth(aes(x= Probability, y= Entropy), 
              method = "lm", formula = y ~ poly(x, 8), se = FALSE) +
  theme_classic() +
  labs(title = "Entropy of weighted coin flip") +
  theme(text=element_text(family="montsr"), 
        plot.title = element_text(size=32),
        plot.subtitle = element_text(size=22),
        axis.title = element_text(size=22),
        axis.text = element_text(size=16),
        legend.text = element_text(size=16), 
        legend.title = element_text(size=22)) +
  scale_color_pilot()

```

For the coin flip (two classes), entropy is constrained between zero and one. A fair coin has the most uncertainty, whereas a coin with some bias towards one side has less uncertainty. This intuitively makes sense. 

## Gini impurity

Gini impurity is one of the other available measures for calculating uncertainty. While entropy does not have an intuitive interpretation of its formula, we can say that gini impurity calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly precisely. It has formula

\begin{align*}
\mbox{Gini index} &= 1 - \sum_{i=1}^{n}{(p_{i})^2}
\end{align*}

where $p_{i}$ is the probability of an element being classified for a distinct class. This can also be easily implemented

```{r include=T}
get_gini_impurity <- function(x){
  #Assume x is a factor with labels
  if(length(x) == 0) return(0)
  weights = table(x)/length(x)
  weights_squared = weights^2
  sum_of_squares = sum(weights_squared)
  gini = 1 - sum_of_squares
  return(gini)
}
```
As with entropy, we can also perform some checks

```{r collapse = T}
#Minimum uncertainty is 0
get_gini_impurity(c(0,0,0,0,0))
#Maximum uncertainty is 0.5 for two classes
get_gini_impurity(c(0,0,1,1))
#Between 0.5 and 1?
get_gini_impurity(c(1,1,2,2,3,3,4,4))
```
Gini impurity in the case of two classes is constrained between zero and half, with zero being minimum uncertainty and half being maximum uncertainty. However with more than two classes, the measure will always be in between zero and one. This is in contrast to entropy which has no upper bound. Once again, note that higher values of gini impurity represent greater uncertainty and vice versa. 

## Information gain

Information gain serves an extension to the calculation of entropy. It is the difference in entropy between a parent node and the average entropy of its children. 

\begin{align*}
\overbrace{\mbox{IG}(T,a)}^{\mbox{information gain}} &= \overbrace{H(T)}^{\mbox{entropy of parent}} - \overbrace{H(T|a)}^{\mbox{average entropy of children}} & \\
\end{align*}

While we seek to minimize entropy, we alternatively seek to maximize information gain. Or in other words, we seek to find the split with the most information gain.

# Stopping conditions

Now that we know how to find the best split (the one that reduces the most uncertainty) and that decision trees essentially recursively split the data into regions, it is important to consider the stopping conditions. This is primarily due to the fact that the CART (Classification and Regression Trees) algorithms are greedy. What I mean by them being 'greedy' is that they will keep splitting the data in an effort to reduce entropy unless told otherwise. This produces a 'locally optimal solution' rather than a 'globally optimal solution'; in simpler terms, we could say that a decision tree never reconsiders its choices, and only makes whatever choice seems best at the moment. To prevent such behaviour we will employ some stopping conditions...

* 


```{r eval = F, include = F, fig.cap = "Decision tree on kyphosis following spinal surgery" }

include_graphics(c("https://upload.wikimedia.org/wikipedia/commons/2/25/Cart_tree_kyphosis.png"), error = F)

```

```{r include = F, eval = F}
**<span style="color: green;">whether this patient has kyphosis?</span>**
```




